{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import Libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from hungarian import * \n",
    "from pprint import pprint \n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%run Radar_Clustering_CustomDBScan.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Path Definition__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\samco\\AppData\\Local\\Temp\\ipykernel_30096\\2588889635.py:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  '''\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nsource_dir = Path(r\"C:\\\\Dk\\\\Projects\\\\Team Project\\\\Dataset\\\\INFRA-3DRC-Dataset\")\\n\\nimage_list = []\\npcd_list = []\\ncalibration_list = []\\nscenes = [f.path for f in os.scandir(source_dir) if f.is_dir()]\\nfor scene in scenes:\\n    # Get the path to the \\'camera_01__data\\' directory\\n    image_dir = Path(scene) / \\'camera_01\\' / \\'camera_01__data\\'\\n    images = sorted(list(image_dir.rglob(\\'*.png\\')))\\n    image_list.append(images)\\n\\n    pcd_dir = Path(scene) / \\'radar_01\\' / \\'radar_01__data\\'\\n    pcds = sorted(list(pcd_dir.rglob(\\'*.pcd\\')))\\n    pcd_list.append(pcds)\\n\\n    calibration_path = Path(scene)\\n    calibration_dir = list(calibration_path.rglob(\\'calibration.json\\'))\\n    calibration_list.extend(calibration_dir)\\n\\nyolo_model = YOLO(r\"C:\\\\Dk\\\\Projects\\\\Team Project\\\\YOLO detection\\\\Models\\\\Harshit_Large\\\\large_300 epoch_batch 4_augmented\\train32\\\\weights\\x08est.pt\")\\ndevice = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\n'"
      ]
     },
     "execution_count": 3168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "source_dir = Path(r\"C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\")\n",
    "\n",
    "image_list = []\n",
    "pcd_list = []\n",
    "calibration_list = []\n",
    "scenes = [f.path for f in os.scandir(source_dir) if f.is_dir()]\n",
    "for scene in scenes:\n",
    "    # Get the path to the 'camera_01__data' directory\n",
    "    image_dir = Path(scene) / 'camera_01' / 'camera_01__data'\n",
    "    images = sorted(list(image_dir.rglob('*.png')))\n",
    "    image_list.append(images)\n",
    "\n",
    "    pcd_dir = Path(scene) / 'radar_01' / 'radar_01__data'\n",
    "    pcds = sorted(list(pcd_dir.rglob('*.pcd')))\n",
    "    pcd_list.append(pcds)\n",
    "\n",
    "    calibration_path = Path(scene)\n",
    "    calibration_dir = list(calibration_path.rglob('calibration.json'))\n",
    "    calibration_list.extend(calibration_dir)\n",
    "\n",
    "yolo_model = YOLO(r\"C:\\Dk\\Projects\\Team Project\\YOLO detection\\Models\\Harshit_Large\\large_300 epoch_batch 4_augmented\\train32\\weights\\best.pt\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_images = Path(r'C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data')\n",
    "path_to_pcd = Path(r'C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\radar_01\\radar_01__data')\n",
    "\n",
    "scene_image = sorted(list(image for image in path_to_images.iterdir()))\n",
    "scene_pcd = sorted(list(image for image in path_to_pcd.iterdir()))\n",
    "\n",
    "# scene_image = scene_image[30:31]\n",
    "# scene_pcd = scene_pcd[30:31]\n",
    "\n",
    "yolo_model = YOLO(r\"C:\\Dk\\Projects\\Team Project\\YOLO detection\\Models\\Harshit_Large\\large_300 epoch_batch 4_augmented\\train32\\weights\\best.pt\")\n",
    "\n",
    "calibration_file = Path(r\"C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\calibration.json\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Function to Process YOLO prediction results__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_box_generator_for_pred(prediction_results):\n",
    "    for result in prediction_results:\n",
    "        cls = result.boxes.cls.cpu().numpy()\n",
    "        conf = result.boxes.conf.cpu().numpy()\n",
    "        detection = result.boxes.xyxy.cpu().numpy()\n",
    "\n",
    "        list_of_pred_boxes = np.column_stack((cls, detection, conf))\n",
    "    \n",
    "    return list_of_pred_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import Calibration Matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensor_calibration_dict(calibration_file):\n",
    "    sensor_calibration_dict = {\n",
    "        \"camera_intrinsics\": [],\n",
    "        \"camera_distcoeffs\": [],\n",
    "        \"radar_to_camera\": [],\n",
    "        \"radar_to_lidar\": [],\n",
    "        \"lidar_to_ground\": [],\n",
    "        \"camera_to_ground\": []\n",
    "    }\n",
    "\n",
    "    with open(calibration_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for item in data['calibration']:\n",
    "        if item['calibration'] == 'camera_01':\n",
    "            sensor_calibration_dict['camera_intrinsics'] = item['k']\n",
    "            sensor_calibration_dict['camera_distcoeffs'] = item['D']\n",
    "        elif item['calibration'] == 'radar_01_to_camera_01':\n",
    "            sensor_calibration_dict['radar_to_camera'] = item['T']\n",
    "        elif item['calibration'] == 'radar_01_to_lidar_01':\n",
    "            sensor_calibration_dict['radar_to_lidar'] = item['T']\n",
    "        elif item['calibration'] == 'lidar_01_to_ground':\n",
    "            sensor_calibration_dict['lidar_to_ground'] = item['T']\n",
    "        elif item['calibration'] == 'camera_01_to_ground_homography':\n",
    "            sensor_calibration_dict['camera_to_ground'] = item['T']\n",
    "\n",
    "    return sensor_calibration_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calibration: Radar to Ground Plane__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_to_ground_transfomer(points_array, T, K):\n",
    "\n",
    "    n_p_array = np.array(points_array).reshape(1,-1)\n",
    "    tranposed_array = np.transpose(n_p_array)\n",
    "   \n",
    "    row_of_ones = np.ones((1, 1))           #1x1\n",
    "    stacked_matrix = np.vstack((tranposed_array, row_of_ones))  \n",
    "  \n",
    "    radar_to_lidar_matrix = np.matmul(T, stacked_matrix)             #3x1\n",
    "\n",
    "    new_stacked_matrix = np.vstack((radar_to_lidar_matrix, row_of_ones))             #4x1\n",
    "    in_ground_data = np.matmul(K, new_stacked_matrix)\n",
    "\n",
    "\n",
    "    in_ground = np.transpose(in_ground_data)\n",
    "\n",
    "    return in_ground[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Radar dict: on Ground__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_to_ground(radar_dict, sensor_calibration_dict):\n",
    "    \n",
    "    T = sensor_calibration_dict['radar_to_lidar']\n",
    "    K = sensor_calibration_dict['lidar_to_ground']\n",
    "\n",
    "    in_radar = radar_dict\n",
    "    in_ground = {'clusters': [], 'noise': []}\n",
    "    for key, value in in_radar.items():\n",
    "        if key == 'clusters':\n",
    "            for point in value:\n",
    "                if point:\n",
    "                    updated_centroid = radar_to_ground_transfomer(point[0], T, K)\n",
    "                    updated_lowest_point = radar_to_ground_transfomer(point[1], T, K)\n",
    "                    updated_velocity = point[2]\n",
    "                    updated_point = [list(updated_centroid), list(updated_lowest_point), list(updated_velocity)]\n",
    "\n",
    "                    if key in in_ground:\n",
    "                        in_ground[key].append(updated_point)\n",
    "                    else:\n",
    "                        print('no key exist')\n",
    "        else:\n",
    "            for point in value:\n",
    "                if point:\n",
    "                    updated_centroid = radar_to_ground_transfomer(point[0], T, K)\n",
    "                    updated_velocity = point[1]\n",
    "                    updated_point = [list(updated_centroid), list(updated_velocity)]\n",
    "\n",
    "                    if key in in_ground:\n",
    "                        in_ground[key].append(updated_point)\n",
    "                    else:\n",
    "                        print('no key exist')\n",
    "                    \n",
    "    return in_ground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calibration: Radar to Image Plane__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_to_camera_transformer(radar_point, T, k):\n",
    "   \n",
    "    n_p_array = np.array(radar_point).reshape(1,-1)\n",
    "    transpose_RPA = np.transpose(n_p_array)\n",
    "\n",
    "    new_array = np.vstack([transpose_RPA, np.ones((1, 1))])             \n",
    "    product_1 = np.matmul(np.array(k), np.array(T))\n",
    "\n",
    "    product_array = np.matmul(product_1, new_array)                      #[su, sv, s] but along column\n",
    "\n",
    "    final_array = product_array / product_array [2]                      #[u, v, 1], along column\n",
    "\n",
    "    u_v = np.delete(final_array, 2, axis = 0)                            #[u, v], along column      \n",
    "    final_u_v = np.transpose(u_v)\n",
    "\n",
    "    return final_u_v[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Radar Dict: on Image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_to_camera(radar_output, sensor_calibration_dict):\n",
    "    T =  sensor_calibration_dict['radar_to_camera']\n",
    "    K = sensor_calibration_dict['camera_intrinsics']\n",
    "    \n",
    "    in_radar = radar_output\n",
    "    in_camera = {'clusters': [], 'noise': []}\n",
    "    for key, value in in_radar.items():\n",
    "        if key == 'clusters':\n",
    "            for point in value:\n",
    "                if point:\n",
    "                    updated_centroid = radar_to_camera_transformer(point[0], T, K)\n",
    "                    updated_lowest_point = radar_to_camera_transformer(point[1], T, K)\n",
    "                    updated_velocity = point[2]\n",
    "                    updated_point = [list(updated_centroid), list(updated_lowest_point), list(updated_velocity)]\n",
    "\n",
    "                    if key in in_camera:\n",
    "                        in_camera[key].append(updated_point)\n",
    "                    else:\n",
    "                        print('no key exist')\n",
    "\n",
    "        if key == 'noise':\n",
    "            for point in value:\n",
    "                if point:\n",
    "                    updated_centroid = radar_to_camera_transformer(point[0], T, K)\n",
    "                    updated_velocity = point[1]\n",
    "                    updated_point = [list(updated_centroid), list(updated_velocity)]\n",
    "\n",
    "                    if key in in_camera:\n",
    "                        in_camera[key].append(updated_point)\n",
    "                    else:\n",
    "                        print('no key exist')\n",
    "\n",
    "    return in_camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Homography: Image to Ground__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef homography2(list_of_pred_boxes):\\n    ground_coordinate_list = []\\n    for result in list_of_pred_boxes:\\n        bbox = list(result[1:5])\\n    \\n        # x1y1 = np.array(bbox[:2]).reshape(1, -1)\\n        # x2y2 = np.array(bbox[2:]).reshape(1, -1)\\n        bottom_center_point = np.array(list(((bbox[2] + bbox[0]) / 2, bbox[3]))).reshape(1, -1) \\n\\n        # image_coordinates = np.concatenate((x1y1,x2y2,bottom_center_point), axis=0)\\n        # image_coordinates = np.concatenate((bottom_center_point), axis=0)\\n        transpose_matrix = np.vstack((np.transpose(bottom_center_point),np.ones((1,1))))\\n        \\n        homogeneous_coordinates = np.matmul(sensor_calibration_dict['camera_to_ground'], transpose_matrix)\\n        ground_coordinates = homogeneous_coordinates / homogeneous_coordinates[-1].reshape(1, -1)\\n\\n        transpose_ground_coordinates = ground_coordinates.T\\n        g_x1y1 = transpose_ground_coordinates[0][:2]\\n        # g_x2y2 = transpose_ground_coordinates[1][:2]\\n        # g_xcyc = transpose_ground_coordinates[2][:2]\\n\\n        # ground_coordinate_list.append([list(g_x1y1), list(g_x2y2), list(g_xcyc)])\\n        ground_coordinate_list.append([list(g_x1y1)])\\n\\n    # print(ground_coordinate_list)\\n    return ground_coordinate_list\\n\\n\""
      ]
     },
     "execution_count": 3176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def homography2(list_of_pred_boxes):\n",
    "    ground_coordinate_list = []\n",
    "    for result in list_of_pred_boxes:\n",
    "        bbox = list(result[1:5])\n",
    "    \n",
    "        # x1y1 = np.array(bbox[:2]).reshape(1, -1)\n",
    "        # x2y2 = np.array(bbox[2:]).reshape(1, -1)\n",
    "        bottom_center_point = np.array(list(((bbox[2] + bbox[0]) / 2, bbox[3]))).reshape(1, -1) \n",
    "\n",
    "        # image_coordinates = np.concatenate((x1y1,x2y2,bottom_center_point), axis=0)\n",
    "        # image_coordinates = np.concatenate((bottom_center_point), axis=0)\n",
    "        transpose_matrix = np.vstack((np.transpose(bottom_center_point),np.ones((1,1))))\n",
    "        \n",
    "        homogeneous_coordinates = np.matmul(sensor_calibration_dict['camera_to_ground'], transpose_matrix)\n",
    "        ground_coordinates = homogeneous_coordinates / homogeneous_coordinates[-1].reshape(1, -1)\n",
    "\n",
    "        transpose_ground_coordinates = ground_coordinates.T\n",
    "        g_x1y1 = transpose_ground_coordinates[0][:2]\n",
    "        # g_x2y2 = transpose_ground_coordinates[1][:2]\n",
    "        # g_xcyc = transpose_ground_coordinates[2][:2]\n",
    "\n",
    "        # ground_coordinate_list.append([list(g_x1y1), list(g_x2y2), list(g_xcyc)])\n",
    "        ground_coordinate_list.append([list(g_x1y1)])\n",
    "\n",
    "    # print(ground_coordinate_list)\n",
    "    return ground_coordinate_list\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homography(points_on_image, sensor_calibration_dict):\n",
    "\n",
    "    points = np.array(points_on_image).reshape(1, -1)\n",
    "\n",
    "    transpose_matrix = np.vstack((np.transpose(points),np.ones((1,1))))\n",
    "    \n",
    "    homogeneous_coordinates = np.matmul(sensor_calibration_dict['camera_to_ground'], transpose_matrix)\n",
    "    ground_coordinates = homogeneous_coordinates / homogeneous_coordinates[-1].reshape(1, -1)\n",
    "\n",
    "    transpose_ground_coordinates = ground_coordinates.T\n",
    "    g_x1y1 = transpose_ground_coordinates[0][:2]\n",
    "\n",
    "    return g_x1y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Visualization: Camera Points on Ground Plane__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_plotting(image_on_ground, my_plot):\n",
    "        x_plotting_list = []\n",
    "        y_plotting_list = []\n",
    "\n",
    "        for xy in image_on_ground:\n",
    "                x_coords = [xy[1][0]]\n",
    "                y_coords = [xy[1][1]]\n",
    "\n",
    "                x_plotting_list.append(x_coords)\n",
    "                y_plotting_list.append(y_coords)\n",
    "        \n",
    "        colors = ['blue', 'green', 'orange', 'black', 'purple', 'maroon']\n",
    "\n",
    "        for i, (x_co, y_co) in enumerate(zip(x_plotting_list, y_plotting_list)):\n",
    "                my_plot.scatter(y_co, x_co, color=colors[i], label= 'camera', marker='o')\n",
    "\n",
    "        my_plot.set_xlim(-30,30)\n",
    "        my_plot.set_ylim(0,100)\n",
    "        my_plot.set_xlabel('Y-axis')\n",
    "        my_plot.set_ylabel('X-axis')\n",
    "        my_plot.set_title('Plot of Points')\n",
    "        my_plot.invert_xaxis()\n",
    "        \n",
    "        return my_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Visualization: Radar points on Ground Plane__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_plotting(dict, my_plot):\n",
    "    clusters = dict['clusters']\n",
    "    noise_points = dict['noise']\n",
    "\n",
    "    x_lowest = []\n",
    "    y_lowest = []\n",
    "\n",
    "    x_noise = []\n",
    "    y_noise = []\n",
    "\n",
    "    for detection in clusters:\n",
    "        if len(detection) != 0:\n",
    "            lowest_point = detection[1]\n",
    "            x_lp = lowest_point[0]\n",
    "            y_lp = lowest_point[1]\n",
    "            x_lowest.append(x_lp)\n",
    "            y_lowest.append(y_lp)\n",
    "    \n",
    "    for noise in noise_points:\n",
    "        if len(noise) != 0:\n",
    "            lowest_point = noise[0]\n",
    "            x_n = lowest_point[0]\n",
    "            y_n = lowest_point[1]\n",
    "            x_noise.append(x_n)\n",
    "            y_noise.append(y_n)\n",
    "\n",
    "\n",
    "    my_plot.scatter(y_lowest, x_lowest, color='red', label='lowest point', marker=\"X\")\n",
    "    my_plot.scatter(y_noise, x_noise, color='grey', label='Noise', marker=\".\")\n",
    "\n",
    "    if not my_plot.get_legend():\n",
    "        my_plot.legend()\n",
    "\n",
    "    return my_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expand Bounding Box Dimension__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_bbox(box, scale=1.1):\n",
    "    # Calculate the width and height of the original box\n",
    "    width = box[2] - box[0]     # x2 - x1\n",
    "    height = box[3] - box[1]    # y2 - y1\n",
    "\n",
    "    # Calculate the center of the original box\n",
    "    center_x = box[0] + (width/2)\n",
    "    center_y = box[1] + (height/2)\n",
    "\n",
    "    # Calculate the increase in width and height\n",
    "    new_width = width * scale\n",
    "    new_height = height * scale\n",
    "\n",
    "    # Calculate the new coordinates\n",
    "    new_x1 = 0 if (center_x - new_width / 2) < 0 else (center_x - new_width / 2)\n",
    "    new_y1 = 0 if (center_y - new_height / 2) < 0 else (center_y - new_height / 2)\n",
    "    new_x2 = center_x + new_width / 2\n",
    "    new_y2 = center_y + new_height / 2\n",
    "    \n",
    "    return list([new_x1, new_y1, new_x2, new_y2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Euclidean Distance__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_euclidean_distance(clusters, images):\n",
    "    d = np.sqrt(((clusters[0] - images[0])**2) + ((clusters[1] - images[1])**2))\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Case Filtering__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_association_matrix(list_of_pred_boxes, cluster_on_image, datatype='clusters', association_list=None):\n",
    "\n",
    "    clusters = list(cluster_on_image['clusters']) if datatype == 'clusters' else list(cluster_on_image['noise'])\n",
    "    pred_boxes = list(list_of_pred_boxes)\n",
    "\n",
    "    if len(clusters) > 0 and len(pred_boxes) > 0:\n",
    "        matrix = np.zeros((len(clusters), len(pred_boxes)))\n",
    "\n",
    "        for pred_idx, prediction in enumerate(pred_boxes):\n",
    "            old_bbox = prediction[1:5]  \n",
    "            bbox = expand_bbox(old_bbox, scale=1.2)\n",
    "\n",
    "            for cluster_idx, cluster in enumerate(clusters):\n",
    "                cluster_centroid = cluster[0]\n",
    "                \n",
    "                if datatype == 'clusters':  \n",
    "                    if bbox[0] < cluster_centroid[0] < bbox[2] and bbox[1] < cluster_centroid[1] < bbox[3]:\n",
    "                        matrix[cluster_idx, pred_idx] = 1\n",
    "                    else: \n",
    "                        matrix[cluster_idx, pred_idx] = 0\n",
    "\n",
    "                elif datatype == 'noise':\n",
    "                    if pred_idx in association_list['non_associated_bbox']:\n",
    "                        if bbox[0] < cluster_centroid[0] < bbox[2] and bbox[1] < cluster_centroid[1] < bbox[3]:\n",
    "                            matrix[cluster_idx, pred_idx] = 1\n",
    "                        else: \n",
    "                            matrix[cluster_idx, pred_idx] = 0\n",
    "\n",
    "        return matrix \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_association_matrix(list_of_pred_boxes, cluster_on_image, datatype='clusters', association_list=None):\\n\\n    if datatype == 'clusters':\\n        clusters = list(cluster_on_image['clusters']) \\n        pred_boxes = list(list_of_pred_boxes)\\n\\n        if len(clusters) > 0 and len(pred_boxes)>0:\\n            matrix = np.zeros((len(clusters), len(pred_boxes))) \\n            for pred_idx, prediction in enumerate(pred_boxes):\\n                \\n                old_bbox = prediction[1:5]  \\n                bbox = expand_bbox(old_bbox, scale=1.2)\\n\\n                for cluster_idx, cluster in enumerate(clusters):\\n                    cluster_centroid = cluster[0]\\n                    \\n                    if bbox[0] < cluster_centroid[0] < bbox[2] and bbox[1] < cluster_centroid[1] < bbox[3]:\\n                        matrix[cluster_idx, pred_idx] = 1\\n\\n                    else: \\n                        matrix[cluster_idx, pred_idx] = 0\\n\\n            return matrix \\n\\n\\n    elif datatype == 'noise':\\n        clusters = list(cluster_on_image['noise'])\\n        pred_boxes = list(list_of_pred_boxes)\\n        # pred_boxes = [list(box) for idx, box in enumerate(list_of_pred_boxes) if idx in association_list['non_associated_bbox']]\\n\\n        if len(clusters) > 0 and len(pred_boxes)>0:\\n            matrix = np.zeros((len(clusters), len(pred_boxes))) \\n            for pred_idx, prediction in enumerate(pred_boxes):\\n                \\n                old_bbox = prediction[1:5]  \\n                bbox = expand_bbox(old_bbox, scale=1.2)\\n\\n                for cluster_idx, cluster in enumerate(clusters):\\n                    cluster_centroid = cluster[0]\\n\\n                    if pred_idx  not in association_list['non_associated_bbox']:\\n                        if bbox[0] < cluster_centroid[0] < bbox[2] and bbox[1] < cluster_centroid[1] < bbox[3]:\\n                            matrix[cluster_idx, pred_idx] = 1\\n\\n                    else: \\n                        matrix[cluster_idx, pred_idx] = 0\\n\\n\\n        return matrix \\n\\n    \\n    else:\\n        print('KeyError')\\n    \""
      ]
     },
     "execution_count": 3183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_association_matrix(list_of_pred_boxes, cluster_on_image, datatype='clusters', association_list=None):\n",
    "\n",
    "    if datatype == 'clusters':\n",
    "        clusters = list(cluster_on_image['clusters']) \n",
    "        pred_boxes = list(list_of_pred_boxes)\n",
    "\n",
    "        if len(clusters) > 0 and len(pred_boxes)>0:\n",
    "            matrix = np.zeros((len(clusters), len(pred_boxes))) \n",
    "            for pred_idx, prediction in enumerate(pred_boxes):\n",
    "                \n",
    "                old_bbox = prediction[1:5]  \n",
    "                bbox = expand_bbox(old_bbox, scale=1.2)\n",
    "\n",
    "                for cluster_idx, cluster in enumerate(clusters):\n",
    "                    cluster_centroid = cluster[0]\n",
    "                    \n",
    "                    if bbox[0] < cluster_centroid[0] < bbox[2] and bbox[1] < cluster_centroid[1] < bbox[3]:\n",
    "                        matrix[cluster_idx, pred_idx] = 1\n",
    "\n",
    "                    else: \n",
    "                        matrix[cluster_idx, pred_idx] = 0\n",
    "\n",
    "            return matrix \n",
    "\n",
    "\n",
    "    elif datatype == 'noise':\n",
    "        clusters = list(cluster_on_image['noise'])\n",
    "        pred_boxes = list(list_of_pred_boxes)\n",
    "        # pred_boxes = [list(box) for idx, box in enumerate(list_of_pred_boxes) if idx in association_list['non_associated_bbox']]\n",
    "\n",
    "        if len(clusters) > 0 and len(pred_boxes)>0:\n",
    "            matrix = np.zeros((len(clusters), len(pred_boxes))) \n",
    "            for pred_idx, prediction in enumerate(pred_boxes):\n",
    "                \n",
    "                old_bbox = prediction[1:5]  \n",
    "                bbox = expand_bbox(old_bbox, scale=1.2)\n",
    "\n",
    "                for cluster_idx, cluster in enumerate(clusters):\n",
    "                    cluster_centroid = cluster[0]\n",
    "\n",
    "                    if pred_idx  not in association_list['non_associated_bbox']:\n",
    "                        if bbox[0] < cluster_centroid[0] < bbox[2] and bbox[1] < cluster_centroid[1] < bbox[3]:\n",
    "                            matrix[cluster_idx, pred_idx] = 1\n",
    "\n",
    "                    else: \n",
    "                        matrix[cluster_idx, pred_idx] = 0\n",
    "\n",
    "\n",
    "        return matrix \n",
    "\n",
    "    \n",
    "    else:\n",
    "        print('KeyError')\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_association_matrix(list_of_pred_boxes, cluster_on_image):\\n\\n    clusters = list(cluster_on_image['clusters'])\\n    # noise_points = list(cluster_on_image['noise'])\\n    pred_boxes = list(list_of_pred_boxes)\\n\\n    if len(clusters) > 0 and len(pred_boxes)>0:\\n        matrix = np.zeros((len(clusters), len(pred_boxes))) \\n        for pred_idx, prediction in enumerate(pred_boxes):\\n            \\n            old_bbox = prediction[1:5]  \\n            bbox = expand_bbox(old_bbox, scale=1.2)\\n\\n            for cluster_idx, cluster in enumerate(clusters):\\n                cluster_centroid = cluster[0]\\n                \\n                if bbox[0] < cluster_centroid[0] < bbox[2] and bbox[1] < cluster_centroid[1] < bbox[3]:\\n                    matrix[cluster_idx, pred_idx] = 1\\n\\n                else: \\n                    matrix[cluster_idx, pred_idx] = 0\\n\\n        return matrix \\n\""
      ]
     },
     "execution_count": 3184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_association_matrix(list_of_pred_boxes, cluster_on_image):\n",
    "\n",
    "    clusters = list(cluster_on_image['clusters'])\n",
    "    # noise_points = list(cluster_on_image['noise'])\n",
    "    pred_boxes = list(list_of_pred_boxes)\n",
    "\n",
    "    if len(clusters) > 0 and len(pred_boxes)>0:\n",
    "        matrix = np.zeros((len(clusters), len(pred_boxes))) \n",
    "        for pred_idx, prediction in enumerate(pred_boxes):\n",
    "            \n",
    "            old_bbox = prediction[1:5]  \n",
    "            bbox = expand_bbox(old_bbox, scale=1.2)\n",
    "\n",
    "            for cluster_idx, cluster in enumerate(clusters):\n",
    "                cluster_centroid = cluster[0]\n",
    "                \n",
    "                if bbox[0] < cluster_centroid[0] < bbox[2] and bbox[1] < cluster_centroid[1] < bbox[3]:\n",
    "                    matrix[cluster_idx, pred_idx] = 1\n",
    "\n",
    "                else: \n",
    "                    matrix[cluster_idx, pred_idx] = 0\n",
    "\n",
    "        return matrix \n",
    "'''      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_noise_association_matrix(association_list, list_of_pred_boxes, cluster_on_image):\\n\\n    # clusters = list(cluster_on_image['clusters'])\\n    noise_points = list(cluster_on_image['noise'])\\n\\n    if not association_list['non_associated_bbox']:\\n        pred_boxes = list(list_of_pred_boxes)\\n    else:\\n        pred_boxes = [list(box) for idx, box in enumerate(list_of_pred_boxes) if idx in association_list['non_associated_bbox']]\\n\\n    pprint(pred_boxes)\\n\\n\\n    if len(noise_points) > 0 and len(pred_boxes)>0:\\n        matrix = np.zeros((len(noise_points), len(pred_boxes))) \\n        for pred_idx, prediction in enumerate(pred_boxes):\\n            \\n            old_bbox = prediction[1:5]  \\n            bbox = expand_bbox(old_bbox, scale=1.2)\\n\\n            for noise_idx, noise in enumerate(noise_points):\\n                noise_centroid = noise[0]\\n                \\n                if bbox[0] < noise_centroid[0] < bbox[2] and bbox[1] < noise_centroid[1] < bbox[3]:\\n                    matrix[noise_idx, pred_idx] = 1\\n\\n                else: \\n                    matrix[noise_idx, pred_idx] = 0\\n\\n        return matrix \\n\\n\""
      ]
     },
     "execution_count": 3185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_noise_association_matrix(association_list, list_of_pred_boxes, cluster_on_image):\n",
    "\n",
    "    # clusters = list(cluster_on_image['clusters'])\n",
    "    noise_points = list(cluster_on_image['noise'])\n",
    "\n",
    "    if not association_list['non_associated_bbox']:\n",
    "        pred_boxes = list(list_of_pred_boxes)\n",
    "    else:\n",
    "        pred_boxes = [list(box) for idx, box in enumerate(list_of_pred_boxes) if idx in association_list['non_associated_bbox']]\n",
    "\n",
    "    pprint(pred_boxes)\n",
    "\n",
    "\n",
    "    if len(noise_points) > 0 and len(pred_boxes)>0:\n",
    "        matrix = np.zeros((len(noise_points), len(pred_boxes))) \n",
    "        for pred_idx, prediction in enumerate(pred_boxes):\n",
    "            \n",
    "            old_bbox = prediction[1:5]  \n",
    "            bbox = expand_bbox(old_bbox, scale=1.2)\n",
    "\n",
    "            for noise_idx, noise in enumerate(noise_points):\n",
    "                noise_centroid = noise[0]\n",
    "                \n",
    "                if bbox[0] < noise_centroid[0] < bbox[2] and bbox[1] < noise_centroid[1] < bbox[3]:\n",
    "                    matrix[noise_idx, pred_idx] = 1\n",
    "\n",
    "                else: \n",
    "                    matrix[noise_idx, pred_idx] = 0\n",
    "\n",
    "        return matrix \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_cases(matrix, datatype='clusters', association_list=None):\n",
    "    \"\"\"\n",
    "    Checks and assigns different cases of radar-image data for spatial association \n",
    "\n",
    "    Examples: \n",
    "    >>> import pprint\n",
    "    >>> matrix = np.array([\n",
    "    ...     [1, 0, 1, 0, 0, 0],\n",
    "    ...     [0, 1, 1, 0, 0, 0],\n",
    "    ...     [1, 1, 0, 0, 0, 0],\n",
    "    ...     [0, 0, 0, 1, 0, 0],\n",
    "    ...     [0, 0, 0, 0, 1, 0],\n",
    "    ...     [0, 0, 0, 0, 1, 0],\n",
    "    ...     [0, 0, 0, 0, 0, 0]\n",
    "    ... ])\n",
    "    >>> pprint.pprint(get_associations(matrix))\n",
    "    {'many_radar_to_many_image': {'cols': [0, 1, 2], 'rows': [0, 1, 2]},\n",
    "     'many_radar_to_one_image': {'cols': [4], 'rows': [(array([4, 5]),)]},\n",
    "     'one_radar_to_many_image': {'cols': [], 'rows': []},\n",
    "     'one_radar_to_one_image': {'cols': [3], 'rows': [3]}}\n",
    "    \"\"\" \n",
    "    associations = {\n",
    "        \"many_cluster_to_many_bbox\" : {\"clusters\": [], \"bbox\": []}, \n",
    "        \"many_cluster_to_one_bbox\"  : {\"clusters\": [], \"bbox\": []},\n",
    "        \"one_cluster_to_many_bbox\"  : {\"clusters\": [], \"bbox\": []}, \n",
    "        \"one_cluster_to_one_bbox\"   : {\"assigned\": []},\n",
    "        \"unassigned_bbox\" : {\"bbox\": []}\n",
    "    }\n",
    "\n",
    "    # MANY TO MANY CHECKS\n",
    "    # -------------------\n",
    "    rows_with_multiple_truths = np.where(np.sum(matrix, axis=1) > 1)[0] \n",
    "    columns_with_multiple_truths = np.where(np.sum(matrix, axis=0) > 1)[0] \n",
    "    # many_too_many = list(set(rows_with_multiple_truths) & set(columns_with_multiple_truths))\n",
    "    # many_radar_to_many_image = [many_too_many, many_too_many]\n",
    "    many_too_many_rows = set() \n",
    "    many_too_many_cols = set() \n",
    "    for r_id in range(matrix.shape[0]):\n",
    "        for c_id in range(matrix.shape[1]):\n",
    "            if r_id in rows_with_multiple_truths and c_id in columns_with_multiple_truths: \n",
    "                if matrix[r_id, c_id] == 1:\n",
    "                     many_too_many_rows.add(r_id)\n",
    "                     many_too_many_cols.add(c_id)\n",
    "\n",
    "    associations['many_cluster_to_many_bbox'][\"clusters\"] = list(many_too_many_rows)\n",
    "    associations['many_cluster_to_many_bbox'][\"bbox\"] = list(many_too_many_cols)\n",
    "\n",
    "    # MANY TO ONE CHECKS \n",
    "    # -------------------\n",
    "    many_to_one = [] \n",
    "    for c in range(matrix.shape[1]): \n",
    "        if c in columns_with_multiple_truths and c not in many_too_many_cols:\n",
    "            associated_rows = np.where(matrix[:, c] > 0)[0]\n",
    "            associations['many_cluster_to_one_bbox'][\"clusters\"].append(associated_rows.tolist())\n",
    "            associations['many_cluster_to_one_bbox'][\"bbox\"].append([c])            \n",
    "\n",
    "    # ONE TO MANY CHECKS\n",
    "    # ------------------\n",
    "    one_to_many = [] \n",
    "    for r in range(matrix.shape[0]): \n",
    "        if r in rows_with_multiple_truths and r not in many_too_many_rows:\n",
    "            associated_cols = np.where(matrix[r] > 0)[0]\n",
    "            associations['one_cluster_to_many_bbox'][\"clusters\"].append([r])\n",
    "            associations['one_cluster_to_many_bbox'][\"bbox\"].append(associated_cols.tolist())\n",
    "\n",
    "    # ONE TO ONE CHECKS\n",
    "    # ------------------\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix[0])):\n",
    "            if matrix[i,j] == 1:\n",
    "                row_sum = sum(matrix[i,:])\n",
    "                col_sum = sum(matrix[:,j]) \n",
    "\n",
    "                if row_sum == 1 and col_sum == 1:\n",
    "                    associations['one_cluster_to_one_bbox']['assigned'].append([i, j]) \n",
    "\n",
    "    if datatype == 'clusters':\n",
    "        associations[\"unassigned_bbox\"][\"bbox\"].extend(list(np.where(np.sum(matrix, axis=0) == 0)[0]))\n",
    "    elif datatype == 'noise':\n",
    "        associations[\"unassigned_bbox\"][\"bbox\"].extend(box for box in list(np.where(np.sum(matrix, axis=0) == 0)[0]) if box in association_list['non_associated_bbox']) \n",
    "\n",
    "    return associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Association: One to One @Image Plane__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_to_one_association(filtered_cases, association_list):\n",
    "    association_list[\"associated\"].extend(filtered_cases['one_cluster_to_one_bbox']['assigned']) \n",
    "    association_list[\"non_associated_bbox\"].extend(filtered_cases[\"unassigned_bbox\"][\"bbox\"]) \n",
    "    return association_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_one_one_association(list_of_pred_boxes, cluster_on_image):\\n    \\n    clusters = list(cluster_on_image[\\'clusters\\'])\\n    noise_points = list(cluster_on_image[\\'noise\\'])\\n    pred_boxes = list(list_of_pred_boxes)\\n\\n    association = {\\'associated\\': [], \\'non_associated\\':{\\'YOLO\\':[], \\'Radar\\':[]}}\\n\\n    if len(clusters) > 0 and len(pred_boxes)>0:\\n        matrix = np.zeros((len(clusters), len(pred_boxes))) \\n        for pred_idx, prediction in enumerate(pred_boxes):\\n            bbox = prediction[1:5]  \\n            for cluster_idx, cluster in enumerate(clusters):\\n                cluster_centroid = cluster[0]\\n                \\n                if bbox[0] < cluster_centroid[0] < bbox[2] and bbox[1] < cluster_centroid[1] < bbox[3]:\\n                    matrix[cluster_idx, pred_idx] = 1\\n\\n                else: \\n                    matrix[cluster_idx, pred_idx] = 0\\n        \\n        pprint(matrix)\\n\\n        special_points = []\\n        for i in range(len(matrix)):\\n            for j in range(len(matrix[0])):\\n                if matrix[i,j] == 1:\\n                    row_sum = sum(matrix[i,:])\\n                    col_sum = sum(matrix[:,j]) \\n\\n                    if row_sum == 1 and col_sum == 1:\\n                        special_points.append((i, j))\\n            \\n        pprint(special_points)\\n\\n        for item in special_points:\\n            association[\\'associated\\'].append([pred_boxes[item[1]],clusters[item[0]]])\\n\\n        for i in range(matrix.shape[0]):\\n            if not any(i == item[0] for item in special_points):\\n                association[\"non_associated\"][\"Radar\"].append(clusters[i])\\n            \\n        for j in range(matrix.shape[1]):\\n            if not any(j == item[1] for item in special_points):\\n                association[\"non_associated\"][\"YOLO\"].append(pred_boxes[j])\\n            \\n                    \\n    return association\\n    '"
      ]
     },
     "execution_count": 3188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_one_one_association(list_of_pred_boxes, cluster_on_image):\n",
    "    \n",
    "    clusters = list(cluster_on_image['clusters'])\n",
    "    noise_points = list(cluster_on_image['noise'])\n",
    "    pred_boxes = list(list_of_pred_boxes)\n",
    "\n",
    "    association = {'associated': [], 'non_associated':{'YOLO':[], 'Radar':[]}}\n",
    "\n",
    "    if len(clusters) > 0 and len(pred_boxes)>0:\n",
    "        matrix = np.zeros((len(clusters), len(pred_boxes))) \n",
    "        for pred_idx, prediction in enumerate(pred_boxes):\n",
    "            bbox = prediction[1:5]  \n",
    "            for cluster_idx, cluster in enumerate(clusters):\n",
    "                cluster_centroid = cluster[0]\n",
    "                \n",
    "                if bbox[0] < cluster_centroid[0] < bbox[2] and bbox[1] < cluster_centroid[1] < bbox[3]:\n",
    "                    matrix[cluster_idx, pred_idx] = 1\n",
    "\n",
    "                else: \n",
    "                    matrix[cluster_idx, pred_idx] = 0\n",
    "        \n",
    "        pprint(matrix)\n",
    "\n",
    "        special_points = []\n",
    "        for i in range(len(matrix)):\n",
    "            for j in range(len(matrix[0])):\n",
    "                if matrix[i,j] == 1:\n",
    "                    row_sum = sum(matrix[i,:])\n",
    "                    col_sum = sum(matrix[:,j]) \n",
    "\n",
    "                    if row_sum == 1 and col_sum == 1:\n",
    "                        special_points.append((i, j))\n",
    "            \n",
    "        pprint(special_points)\n",
    "\n",
    "        for item in special_points:\n",
    "            association['associated'].append([pred_boxes[item[1]],clusters[item[0]]])\n",
    "\n",
    "        for i in range(matrix.shape[0]):\n",
    "            if not any(i == item[0] for item in special_points):\n",
    "                association[\"non_associated\"][\"Radar\"].append(clusters[i])\n",
    "            \n",
    "        for j in range(matrix.shape[1]):\n",
    "            if not any(j == item[1] for item in special_points):\n",
    "                association[\"non_associated\"][\"YOLO\"].append(pred_boxes[j])\n",
    "            \n",
    "                    \n",
    "    return association\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Association: One to Many @Ground Plane__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_to_many_association(filtered_cases, clusters_on_ground, image_on_ground, datatype='clusters', association_list=None):\n",
    "\n",
    "    if datatype == 'clusters':\n",
    "        lower_idx = 1\n",
    "    elif datatype == 'noise':\n",
    "        lower_idx = 0\n",
    "    else:\n",
    "        print('KeyError')\n",
    "\n",
    "    list_of_centroid_indices = filtered_cases['one_cluster_to_many_bbox']['clusters']\n",
    "    list_of_box_indices = filtered_cases['one_cluster_to_many_bbox']['bbox']\n",
    "\n",
    "    for centroid, bboxes in zip(list_of_centroid_indices, list_of_box_indices):\n",
    "        centroid = centroid[0]\n",
    "\n",
    "        euclidean_distances = []\n",
    "        for box in bboxes:\n",
    "            ec_distance = get_euclidean_distance(clusters_on_ground[datatype][centroid][lower_idx], image_on_ground[box][1])\n",
    "            euclidean_distances.append(ec_distance)\n",
    "        \n",
    "        min_bbox_idx = [idx for idx, value in enumerate(euclidean_distances) if value == min(euclidean_distances)]\n",
    "        min_bbox_idx = min_bbox_idx[0]\n",
    "\n",
    "        association_list['associated'].append([centroid, bboxes[min_bbox_idx]])\n",
    "        association_list['non_associated_bbox'].extend([bbox for bbox in bboxes if bbox != bboxes[min_bbox_idx]])\n",
    "\n",
    "    return association_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Association: Many to One__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_point_finder(points_list, processed_radar_points_to_ground):\n",
    "    x_list = []\n",
    "    for point in points_list:\n",
    "        x_p = processed_radar_points_to_ground['clusters'][point][1][0]\n",
    "        x_list.append(x_p)\n",
    "    \n",
    "    return x_list.index(min(x_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_many_to_one_association(filtered_cases, association_list, clusters_on_ground, image_on_ground, datatype='clusters'):\n",
    "\n",
    "    clusters = filtered_cases['many_cluster_to_one_bbox']['clusters']\n",
    "    pre_boxes = filtered_cases['many_cluster_to_one_bbox']['bbox']\n",
    "\n",
    "  \n",
    "    for cluster, box in zip(clusters, pre_boxes):\n",
    "    # if count_in_clusters == 1 and count_in_boxes == 1:\n",
    "        box_data = box[0]\n",
    "        clusters_data = cluster\n",
    "        count = len(clusters_data)\n",
    "        matrix = np.zeros((count, count))\n",
    "\n",
    "        for index_p1, point_1 in enumerate(clusters_data):\n",
    "            for index_p2, point_2 in enumerate(clusters_data):\n",
    "                point_1_data = clusters_on_ground['clusters'][point_1] if datatype == 'clusters' else clusters_on_ground['noise'][point_1]\n",
    "                point_2_data = clusters_on_ground['clusters'][point_2] if datatype == 'clusters' else clusters_on_ground['noise'][point_2]\n",
    "\n",
    "                velocity_p1 = point_1_data[2][0] if datatype == 'clusters' else point_1_data[1][0]\n",
    "                velocity_p2 = point_2_data[2][0] if datatype == 'clusters' else point_2_data[1][0]\n",
    "\n",
    "                x_p1 = point_1_data[0][0]\n",
    "                x_p2 = point_2_data[0][0]\n",
    "                \n",
    "                if abs(velocity_p1 - velocity_p2) < 0.75 and abs(x_p1 - x_p2) < 2:\n",
    "                    matrix[index_p1][index_p2] = 1\n",
    "\n",
    "        candidates = []\n",
    "        for row in matrix:\n",
    "            columns_with_ones = []\n",
    "            for col_index, value in enumerate(row):\n",
    "                if value == 1:\n",
    "                    columns_with_ones.append(col_index)\n",
    "        \n",
    "            candidates.append(columns_with_ones)\n",
    "\n",
    "        new_candidates = candidates\n",
    "        global_merged_any = False\n",
    "        while True:\n",
    "            #print(new_candidates)\n",
    "            check_list = new_candidates\n",
    "            for i in range(len(new_candidates)):\n",
    "                local_merged_any = False\n",
    "                set1 = set(new_candidates[i])\n",
    "                for j in range(i + 1, len(new_candidates)):\n",
    "                    set2 = set(new_candidates[j])\n",
    "\n",
    "                    if set1 & set2:  # Check if there's any common element\n",
    "                        new_element = list(set1 | set2)\n",
    "                        remaining_points = [point for point in new_candidates if point not in [new_candidates[i], new_candidates[j]]]\n",
    "                        new_candidates = remaining_points\n",
    "                        new_candidates.insert(0, new_element) # New candidate is not emptying --no break condition \n",
    "\n",
    "                        local_merged_any = True \n",
    "                        global_merged_any = True\n",
    "                        break\n",
    "                if local_merged_any:\n",
    "                    break\n",
    "\n",
    "            # Exit condition for the while loop\n",
    "            if len(new_candidates) == len(check_list):\n",
    "                break\n",
    "                    \n",
    "        if not global_merged_any:  # If no merges occurred in this iteration, terminate the loop\n",
    "            distance_data = []\n",
    "            for idx, centroid in enumerate(clusters_data):\n",
    "                nearest_data = clusters_on_ground['clusters'][centroid][1] if datatype == 'clusters' else clusters_on_ground['noise'][centroid][0]\n",
    "                box_bottom_center = image_on_ground[box_data][1]\n",
    "                ec_distance = get_euclidean_distance(nearest_data[0:2], box_bottom_center)\n",
    "                distance_data.append(ec_distance)\n",
    "            \n",
    "            index_to_look_for_in_clusters_data = distance_data.index(min(distance_data))\n",
    "            index_of_chosen_centroid = clusters_data[index_to_look_for_in_clusters_data]\n",
    "\n",
    "            if [index_of_chosen_centroid, box_data] not in association_list['associated']:\n",
    "                association_list['associated'].append([index_of_chosen_centroid, box_data])\n",
    "            print('not merged case')\n",
    "\n",
    "        new_updated_candidate = []\n",
    "        for box_list in new_candidates:\n",
    "            updated = []\n",
    "            for box in box_list:\n",
    "                updated.append(clusters_data[box]) \n",
    "            new_updated_candidate.append(updated)\n",
    "            \n",
    "        if len(new_updated_candidate) > 1 and global_merged_any:\n",
    "\n",
    "            #This section removes the items which are not clustered\n",
    "            for pair in new_updated_candidate[:]:\n",
    "                if len(pair) < 2:\n",
    "                    new_updated_candidate.remove(pair)    \n",
    "\n",
    "            distance_comparison = []\n",
    "            for item in new_updated_candidate:\n",
    "                nearest_point = nearest_point_finder(item, clusters_on_ground)\n",
    "                nearest_point_data = clusters_on_ground['clusters'][item[nearest_point]][1] if datatype == 'clusters' else clusters_on_ground['noise'][item[nearest_point]][0]\n",
    "                box_bottom_center = image_on_ground[box_data][1]\n",
    "                e_c_distance = get_euclidean_distance(nearest_point_data[0:2], box_bottom_center)\n",
    "                distance_comparison.append(e_c_distance)\n",
    "\n",
    "            final_cluster_to_associate= distance_comparison.index(min(distance_comparison))\n",
    "            index_of_chosen_centroid = new_updated_candidate[final_cluster_to_associate]\n",
    "\n",
    "            #print(final_candidate, box_data)\n",
    "            association_list['associated'].append([index_of_chosen_centroid, box_data])\n",
    "\n",
    "        elif len(new_updated_candidate) == 1:\n",
    "            association_list['associated'].append([new_updated_candidate[0], box_data])\n",
    "        \n",
    "    return association_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Many to Many__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_many_to_many_association(filtered_cases, association_list, clusters_on_ground, image_on_ground, datatype='clusters'):\n",
    "\n",
    "    clusters = filtered_cases['many_cluster_to_one_bbox']['clusters']\n",
    "    pre_boxes = filtered_cases['many_cluster_to_one_bbox']['bbox']\n",
    "\n",
    "    \n",
    "    first_matrix = np.zeros((len(clusters), len(clusters)))\n",
    "\n",
    "    for index_p1, point_1 in enumerate(clusters):\n",
    "            for index_p2, point_2 in enumerate(clusters):\n",
    "                point_1_data = clusters_on_ground['clusters'][point_1] if datatype == 'clusters' else clusters_on_ground['noise'][point_1]\n",
    "                point_2_data = clusters_on_ground['clusters'][point_2] if datatype == 'clusters' else clusters_on_ground['noise'][point_2]\n",
    "\n",
    "                velocity_p1 = point_1_data[2][0] if datatype == 'clusters' else point_1_data[1][0]\n",
    "                velocity_p2 = point_2_data[2][0] if datatype == 'clusters' else point_2_data[1][0]\n",
    "                \n",
    "                if abs(velocity_p1 - velocity_p2) < 0.75:\n",
    "                    first_matrix[index_p1][index_p2] = 1\n",
    "    \n",
    "    # print(first_matrix)\n",
    "\n",
    "    candidates = []\n",
    "    for row in first_matrix:\n",
    "        columns_with_ones = []\n",
    "        for col_index, value in enumerate(row):\n",
    "            if value == 1:\n",
    "                columns_with_ones.append(col_index)\n",
    "    \n",
    "        candidates.append(columns_with_ones)\n",
    "\n",
    "    #########################################################################################################################\n",
    "    new_candidates = candidates\n",
    "    global_merged_any = False\n",
    "    while True:\n",
    "        #print(new_candidates)\n",
    "        check_list = new_candidates\n",
    "        for i in range(len(new_candidates)):\n",
    "            local_merged_any = False\n",
    "            set1 = set(new_candidates[i])\n",
    "            for j in range(i + 1, len(new_candidates)):\n",
    "                set2 = set(new_candidates[j])\n",
    "\n",
    "                if set1 & set2:  # Check if there's any common element\n",
    "                    new_element = list(set1 | set2)\n",
    "                    remaining_points = [point for point in new_candidates if point not in [new_candidates[i], new_candidates[j]]]\n",
    "                    new_candidates = remaining_points\n",
    "                    new_candidates.insert(0, new_element) # New candidate is not emptying --no break condition \n",
    "\n",
    "                    local_merged_any = True \n",
    "                    global_merged_any = True\n",
    "                    break\n",
    "            if local_merged_any:\n",
    "                break\n",
    "\n",
    "        # Exit condition for the while loop\n",
    "        if len(new_candidates) == len(check_list):\n",
    "            break\n",
    "\n",
    "    ################################################################################################################\n",
    "    if not global_merged_any:  # If no merges occurred in this iteration, terminate the loop\n",
    "\n",
    "        second_matrix = np.zeros((len(clusters), len(pre_boxes)))\n",
    "\n",
    "        for cluster in clusters:\n",
    "            for box in pre_boxes:\n",
    "                cluster_nearest_point_data = clusters_on_ground['clusters'][cluster][1]  if datatype == 'clusters' else clusters_on_ground['noise'][cluster][0]\n",
    "                box_bottom_center_data = image_on_ground[box][1]\n",
    "                e_c_distance = get_euclidean_distance(cluster_nearest_point_data[0:2], box_bottom_center_data)\n",
    "                second_matrix[cluster][box] = e_c_distance\n",
    "        \n",
    "        n_rows, n_cols = second_matrix.shape\n",
    "        combined_indices = []\n",
    "\n",
    "        for col_idx in range(n_cols):\n",
    "            # Get the column and apply the threshold condition\n",
    "            column = second_matrix.shape[:, col_idx]\n",
    "            valid_indices = np.where(column < 2)[0]\n",
    "            \n",
    "            if valid_indices.size > 0:\n",
    "                # Find the index of the minimum value among valid indices\n",
    "                min_value_index = valid_indices[np.argmin(column[valid_indices])]\n",
    "                combined_indices.append((min_value_index, col_idx))\n",
    "        \n",
    "        # print(combined_indices)             #list of associations where first element is cluster index and second is box index \n",
    "        for item in combined_indices:\n",
    "            new_item = clusters[item[0]], pre_boxes[item[1]]\n",
    "        association_list['associated'].extend(new_item)\n",
    "\n",
    "    #####################################################################################################################\n",
    "\n",
    "    #getting the actual indexes\n",
    "    new_updated_candidate = []\n",
    "    for box_list in new_candidates:\n",
    "        updated = []\n",
    "        for box in box_list:\n",
    "            updated.append(clusters[box]) \n",
    "        new_updated_candidate.append(updated)\n",
    "\n",
    "    if global_merged_any:\n",
    "        number_of_rows = len(new_candidates)\n",
    "        len_of_columns = len(pre_boxes)\n",
    "\n",
    "        third_matrix = np.zeros((number_of_rows, len_of_columns))\n",
    "\n",
    "        for merged in new_candidates:\n",
    "            for box in pre_boxes:\n",
    "                if len(merged) > 1:\n",
    "                    nearest_point_index_in_merged = nearest_point_finder(merged, image_on_ground)\n",
    "                    nearest_point = merged[nearest_point_index_in_merged]\n",
    "                else:\n",
    "                    nearest_point = merged[0]\n",
    "                \n",
    "                nearest_point_data = clusters_on_ground['cluster'][nearest_point][1] if datatype == 'clusters' else clusters_on_ground['noise'][nearest_point][0]\n",
    "                box_bottom_center = image_on_ground[box][1]\n",
    "                e_c_distance = get_euclidean_distance(nearest_point_data[0:2], box_bottom_center)\n",
    "                third_matrix[merged][box] = e_c_distance\n",
    "        \n",
    "\n",
    "        n_rows, n_cols = third_matrix.shape\n",
    "        combined_indices = []\n",
    "\n",
    "        for col_idx in range(n_cols):\n",
    "            # Get the column and apply the threshold condition\n",
    "            column = third_matrix.shape[:, col_idx]\n",
    "            valid_indices = np.where(column < 2)[0]\n",
    "            \n",
    "            if valid_indices.size > 0:\n",
    "                # Find the index of the minimum value among valid indices\n",
    "                min_value_index = valid_indices[np.argmin(column[valid_indices])]\n",
    "                combined_indices.append((min_value_index, col_idx))\n",
    "        \n",
    "        # print(combined_indices)             #list of associations where first element is cluster index and second is box index \n",
    "\n",
    "        for item in combined_indices:\n",
    "            new_item = new_candidates[item[0]], pre_boxes[item[1]]\n",
    "        association_list['associated'].extend(new_item)\n",
    "    \n",
    "    for boxes in pre_boxes:\n",
    "        if boxes not in list(item[1] for item in association_list['associated']):\n",
    "            association_list['non_associated'].append(boxes)\n",
    "\n",
    "    return association_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Jayesh__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def find_and_group_similar_velocities(radar_on_camera, velocities, threshold=0.5, datatype='clusters'):\\n        grouped_points = []\\n        used_indices = set()\\n        if datatype == 'noise': \\n            points = radar_on_camera['noise']\\n            v_index = 1 # Due to different structure, the position of velocity will be different\\n        else: \\n            points = radar_on_camera['clusters']\\n            v_index = 2\\n\\n        for i in velocities:\\n            if i not in used_indices:\\n                current_group = [i]\\n                used_indices.add(i)\\n                for j in velocities[i+1:]:\\n                    if abs(points[i][v_index][0] - points[j][v_index][0]) <= threshold:\\n                        current_group.append(j)\\n                        used_indices.add(j)\\n                grouped_points.append(current_group)\\n        \\n        return grouped_points\""
      ]
     },
     "execution_count": 3193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def find_and_group_similar_velocities(radar_on_camera, velocities, threshold=0.5, datatype='clusters'):\n",
    "        grouped_points = []\n",
    "        used_indices = set()\n",
    "        if datatype == 'noise': \n",
    "            points = radar_on_camera['noise']\n",
    "            v_index = 1 # Due to different structure, the position of velocity will be different\n",
    "        else: \n",
    "            points = radar_on_camera['clusters']\n",
    "            v_index = 2\n",
    "\n",
    "        for i in velocities:\n",
    "            if i not in used_indices:\n",
    "                current_group = [i]\n",
    "                used_indices.add(i)\n",
    "                for j in velocities[i+1:]:\n",
    "                    if abs(points[i][v_index][0] - points[j][v_index][0]) <= threshold:\n",
    "                        current_group.append(j)\n",
    "                        used_indices.add(j)\n",
    "                grouped_points.append(current_group)\n",
    "        \n",
    "        return grouped_points'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def merge_clusters(clusters_on_radar, clusters, datatype=\\'clusters\\'):\\n        avg_centroid = np.empty((1,3))\\n        avg_lowest_point = np.empty((1,3))\\n        avg_velocity = 0\\n        if datatype==\\'noise\\': \\n            points = clusters_on_radar[\\'noise\\']\\n            v_index = 1\\n        else: \\n            points = clusters_on_radar[\\'clusters\\']\\n            v_index = 2\\n        \\n        for cluster in clusters:\\n            # print(f\"Cluster: {cluster}\")\\n            # print(f\"Points: {points}\")\\n            avg_centroid += np.array(points[cluster][0])\\n            avg_lowest_point += np.array(points[cluster][1])\\n            avg_velocity += points[cluster][v_index][0] \\n        \\n        avg_centroid /= len(clusters)\\n        avg_lowest_point /= len(clusters)\\n        avg_velocity /= len(clusters)\\n\\n        merged_cluster = [avg_centroid.tolist()[0], avg_lowest_point.tolist()[0], [avg_velocity]]\\n\\n        return merged_cluster\\n'"
      ]
     },
     "execution_count": 3194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def merge_clusters(clusters_on_radar, clusters, datatype='clusters'):\n",
    "        avg_centroid = np.empty((1,3))\n",
    "        avg_lowest_point = np.empty((1,3))\n",
    "        avg_velocity = 0\n",
    "        if datatype=='noise': \n",
    "            points = clusters_on_radar['noise']\n",
    "            v_index = 1\n",
    "        else: \n",
    "            points = clusters_on_radar['clusters']\n",
    "            v_index = 2\n",
    "        \n",
    "        for cluster in clusters:\n",
    "            # print(f\"Cluster: {cluster}\")\n",
    "            # print(f\"Points: {points}\")\n",
    "            avg_centroid += np.array(points[cluster][0])\n",
    "            avg_lowest_point += np.array(points[cluster][1])\n",
    "            avg_velocity += points[cluster][v_index][0] \n",
    "        \n",
    "        avg_centroid /= len(clusters)\n",
    "        avg_lowest_point /= len(clusters)\n",
    "        avg_velocity /= len(clusters)\n",
    "\n",
    "        merged_cluster = [avg_centroid.tolist()[0], avg_lowest_point.tolist()[0], [avg_velocity]]\n",
    "\n",
    "        return merged_cluster\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_many_to_many(filtered_cases, clusters_on_image, velocity_threshold=0.75, datatype=\\'clusters\\')\\n    if datatype == \\'clusters\\':\\n        merged_clusters_indexes = find_and_group_similar_velocities(clusters_on_image, filtered_cases[\\'many_cluster_to_many_bbox\\'][\"clusters\"], threshold=0.5, datatype=\\'clusters\\')\\n        merged_clusters_indexes_copy = merged_clusters_indexes\\n\\n        merged_clusters = [merge_clusters(clusters) for clusters in merged_clusters_indexes if len(clusters)>1]\\n'"
      ]
     },
     "execution_count": 3195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_many_to_many(filtered_cases, clusters_on_image, velocity_threshold=0.75, datatype='clusters')\n",
    "    if datatype == 'clusters':\n",
    "        merged_clusters_indexes = find_and_group_similar_velocities(clusters_on_image, filtered_cases['many_cluster_to_many_bbox'][\"clusters\"], threshold=0.5, datatype='clusters')\n",
    "        merged_clusters_indexes_copy = merged_clusters_indexes\n",
    "\n",
    "        merged_clusters = [merge_clusters(clusters) for clusters in merged_clusters_indexes if len(clusters)>1]\n",
    "'''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_noise_one_to_many_association(filtered_cases, noise_association_list, clusters_on_ground, image_on_ground):\\n\\n    list_of_noise_indices = filtered_cases['one_cluster_to_many_bbox']['clusters']\\n    list_of_box_indices = filtered_cases['one_cluster_to_many_bbox']['bbox']\\n\\n    for centroid, bboxes in zip(list_of_noise_indices, list_of_box_indices):\\n        centroid = centroid[0]\\n\\n        euclidean_distances = []\\n        for box in bboxes:\\n            ec_distance = get_euclidean_distance(clusters_on_ground['noise'][centroid][0], image_on_ground[box][1])\\n            euclidean_distances.append(ec_distance)\\n        \\n        min_bbox_idx = [idx for idx, value in enumerate(euclidean_distances) if value == min(euclidean_distances)]\\n        min_bbox_idx = min_bbox_idx[0]\\n\\n        noise_association_list['associated'].append([centroid, bboxes[min_bbox_idx]])\\n        noise_association_list['non_associated_bbox'].extend([bbox for bbox in bboxes if bbox != bboxes[min_bbox_idx]])\\n\\n    return noise_association_list\\n\\n    \""
      ]
     },
     "execution_count": 3196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_noise_one_to_many_association(filtered_cases, noise_association_list, clusters_on_ground, image_on_ground):\n",
    "\n",
    "    list_of_noise_indices = filtered_cases['one_cluster_to_many_bbox']['clusters']\n",
    "    list_of_box_indices = filtered_cases['one_cluster_to_many_bbox']['bbox']\n",
    "\n",
    "    for centroid, bboxes in zip(list_of_noise_indices, list_of_box_indices):\n",
    "        centroid = centroid[0]\n",
    "\n",
    "        euclidean_distances = []\n",
    "        for box in bboxes:\n",
    "            ec_distance = get_euclidean_distance(clusters_on_ground['noise'][centroid][0], image_on_ground[box][1])\n",
    "            euclidean_distances.append(ec_distance)\n",
    "        \n",
    "        min_bbox_idx = [idx for idx, value in enumerate(euclidean_distances) if value == min(euclidean_distances)]\n",
    "        min_bbox_idx = min_bbox_idx[0]\n",
    "\n",
    "        noise_association_list['associated'].append([centroid, bboxes[min_bbox_idx]])\n",
    "        noise_association_list['non_associated_bbox'].extend([bbox for bbox in bboxes if bbox != bboxes[min_bbox_idx]])\n",
    "\n",
    "    return noise_association_list\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Make Association Matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_association_matrix(image_points_on_ground, radar_points):\\n    clusters = radar_points['clusters']\\n    noise_points = radar_points['noise']\\n    \\n    cluster_nearest_points = []\\n    noise_nearest_points = []\\n\\n    for detection in clusters: \\n        if len(detection) != 0:\\n            lowest_point = detection[1]\\n            x_lp = lowest_point[0]\\n            y_lp = lowest_point[1]\\n            cluster_nearest_points.append([list(np.array([x_lp, y_lp]))]) \\n    \\n    for noise in noise_points: \\n        if len(noise) != 0:\\n            lowest_point = noise[0]\\n            x_n = lowest_point[0]\\n            y_n = lowest_point[1]\\n            noise_nearest_points.append([list(np.array([x_n, y_n]))])     \\n    \\n\\n    total_list = cluster_nearest_points + noise_nearest_points\\n\\n    association_matrix = np.zeros((len(total_list),len(image_points_on_ground)))\\n\\n    for cluster_idx, cluster_point in enumerate(total_list):\\n        for img_idx, img_point in enumerate(image_points_on_ground):\\n            association_matrix[cluster_idx, img_idx] = get_euclidean_distance(cluster_point[0], img_point[0])\\n\\n    \\n    return association_matrix   \\n\\n\""
      ]
     },
     "execution_count": 3197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_association_matrix(image_points_on_ground, radar_points):\n",
    "    clusters = radar_points['clusters']\n",
    "    noise_points = radar_points['noise']\n",
    "    \n",
    "    cluster_nearest_points = []\n",
    "    noise_nearest_points = []\n",
    "\n",
    "    for detection in clusters: \n",
    "        if len(detection) != 0:\n",
    "            lowest_point = detection[1]\n",
    "            x_lp = lowest_point[0]\n",
    "            y_lp = lowest_point[1]\n",
    "            cluster_nearest_points.append([list(np.array([x_lp, y_lp]))]) \n",
    "    \n",
    "    for noise in noise_points: \n",
    "        if len(noise) != 0:\n",
    "            lowest_point = noise[0]\n",
    "            x_n = lowest_point[0]\n",
    "            y_n = lowest_point[1]\n",
    "            noise_nearest_points.append([list(np.array([x_n, y_n]))])     \n",
    "    \n",
    "\n",
    "    total_list = cluster_nearest_points + noise_nearest_points\n",
    "\n",
    "    association_matrix = np.zeros((len(total_list),len(image_points_on_ground)))\n",
    "\n",
    "    for cluster_idx, cluster_point in enumerate(total_list):\n",
    "        for img_idx, img_point in enumerate(image_points_on_ground):\n",
    "            association_matrix[cluster_idx, img_idx] = get_euclidean_distance(cluster_point[0], img_point[0])\n",
    "\n",
    "    \n",
    "    return association_matrix   \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Greedy Object Association__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef assign_objects(_matrix):\\n    \\n    for i_dx in range(_matrix.shape[0]):\\n        _matrix[i_dx,][_matrix[i_dx,] != np.max(_matrix[i_dx,])] = 0\\n\\n    for i_gx in range(_matrix.shape[1]):\\n        _matrix[:, i_gx][_matrix[:,i_gx] != np.max(_matrix[:,i_gx])] = 0 \\n\\n    return _matrix \\n'"
      ]
     },
     "execution_count": 3198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def assign_objects(_matrix):\n",
    "    \n",
    "    for i_dx in range(_matrix.shape[0]):\n",
    "        _matrix[i_dx,][_matrix[i_dx,] != np.max(_matrix[i_dx,])] = 0\n",
    "\n",
    "    for i_gx in range(_matrix.shape[1]):\n",
    "        _matrix[:, i_gx][_matrix[:,i_gx] != np.max(_matrix[:,i_gx])] = 0 \n",
    "\n",
    "    return _matrix \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Update Dict: Image Plane to Ground Plane__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef update_dict_image_ground(dictionary_after_1t1, sensor_calibration_dict):\\n    _to_ground_dict = {'associated': [], 'non_associated': {'YOLO': [], 'Radar': []}}\\n    \\n    for result in dictionary_after_1t1['non_associated']['YOLO']:\\n        cls = result[0]\\n        bbox = list(result[1:5])\\n        bottom_center_point = list(((bbox[2] + bbox[0]) / 2, bbox[3]))\\n        image_point_on_ground = list(homography(bottom_center_point, sensor_calibration_dict))\\n        _to_ground_dict['non_associated']['YOLO'].append([[cls], image_point_on_ground])\\n\\n\\n    for cluster in dictionary_after_1t1['non_associated']['Radar']:\\n        centroid = cluster[0]\\n        lower = cluster[1]\\n        velocity = cluster[2]\\n        _centroid = list(homography(centroid,sensor_calibration_dict))\\n        _lower = list(homography(lower, sensor_calibration_dict))\\n        _to_ground_dict['non_associated']['Radar'].append([_centroid, _lower, velocity])\\n\\n\\n    for item in dictionary_after_1t1['associated']:\\n        cls = item[0][0]\\n        bbox_lower = item[0][1]\\n        radar_data = item[1]\\n        centroid = radar_data[0]\\n        velocity = radar_data[2]\\n        _centroid = list(homography(centroid, sensor_calibration_dict))\\n        _to_ground_dict['associated'].append([[cls], _centroid, velocity])\\n\\n    return _to_ground_dict\\n\""
      ]
     },
     "execution_count": 3199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def update_dict_image_ground(dictionary_after_1t1, sensor_calibration_dict):\n",
    "    _to_ground_dict = {'associated': [], 'non_associated': {'YOLO': [], 'Radar': []}}\n",
    "    \n",
    "    for result in dictionary_after_1t1['non_associated']['YOLO']:\n",
    "        cls = result[0]\n",
    "        bbox = list(result[1:5])\n",
    "        bottom_center_point = list(((bbox[2] + bbox[0]) / 2, bbox[3]))\n",
    "        image_point_on_ground = list(homography(bottom_center_point, sensor_calibration_dict))\n",
    "        _to_ground_dict['non_associated']['YOLO'].append([[cls], image_point_on_ground])\n",
    "\n",
    "\n",
    "    for cluster in dictionary_after_1t1['non_associated']['Radar']:\n",
    "        centroid = cluster[0]\n",
    "        lower = cluster[1]\n",
    "        velocity = cluster[2]\n",
    "        _centroid = list(homography(centroid,sensor_calibration_dict))\n",
    "        _lower = list(homography(lower, sensor_calibration_dict))\n",
    "        _to_ground_dict['non_associated']['Radar'].append([_centroid, _lower, velocity])\n",
    "\n",
    "\n",
    "    for item in dictionary_after_1t1['associated']:\n",
    "        cls = item[0][0]\n",
    "        bbox_lower = item[0][1]\n",
    "        radar_data = item[1]\n",
    "        centroid = radar_data[0]\n",
    "        velocity = radar_data[2]\n",
    "        _centroid = list(homography(centroid, sensor_calibration_dict))\n",
    "        _to_ground_dict['associated'].append([[cls], _centroid, velocity])\n",
    "\n",
    "    return _to_ground_dict\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Association Visualization @Image Plane__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drawings(colour_idx, image, box_data, centroid_data, datatype='clusters'):\n",
    "\n",
    "    colors = [\n",
    "        (255, 0, 0),    # Blue\n",
    "        (0, 255, 0),    # Green\n",
    "        (0, 0, 255),    # Red\n",
    "        (255, 255, 0),  # Cyan\n",
    "        (255, 0, 255),  # Magenta\n",
    "        (0, 255, 255),  # Yellow\n",
    "        (128, 0, 0),    # Maroon\n",
    "        (0, 128, 0),    # Dark Green\n",
    "        (0, 0, 128),    # Navy\n",
    "        (128, 128, 0)   # Olive\n",
    "    ]\n",
    "\n",
    "    # Calculate the Corners\n",
    "    original_box = box_data[1:5]\n",
    "    # pprint(original_box)\n",
    "    corner_1 = tuple(map(int, [original_box[0], original_box[1]]))\n",
    "    corner_2 = tuple(map(int, [original_box[2], original_box[3]]))            \n",
    "\n",
    "    # Draw Original bounding box\n",
    "    colour = colors[colour_idx]\n",
    "    thickness_of_box = 3\n",
    "    cv2.rectangle(image, corner_1, corner_2, colour, thickness_of_box)\n",
    "\n",
    "    # expand bounding boxes and calculate the corners\n",
    "    new_box = expand_bbox(box_data[1:5], scale=1.2) \n",
    "    corner_1 = tuple(map(int, [new_box[0], new_box[1]]))\n",
    "    corner_2 = tuple(map(int, [new_box[2], new_box[3]]))\n",
    "    \n",
    "\n",
    "    # Draw a Expanded bounding box with dashed Rectangle \n",
    "    img_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    thickness = 3\n",
    "    dash_length = 10\n",
    "\n",
    "    for i in range(corner_1[0], corner_2[0], dash_length * 2):\n",
    "        for t in range(thickness):\n",
    "            draw.line([(i, corner_1[1]+t), (i + dash_length, corner_1[1]+t)], fill=colour[::-1])\n",
    "            draw.line([(i, corner_2[1]+t), (i + dash_length, corner_2[1]+t)], fill=colour[::-1])\n",
    "\n",
    "    for i in range(corner_1[1], corner_2[1], dash_length * 2):\n",
    "        for t in range(thickness):\n",
    "            draw.line([(corner_1[0]+t, i), (corner_1[0]+t, i + dash_length)], fill=colour[::-1])\n",
    "            draw.line([(corner_2[0]+t, i), (corner_2[0]+t, i + dash_length)], fill=colour[::-1])\n",
    "\n",
    "    image = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    \n",
    "    # Plot Centroid\n",
    "    thickness = -1  # to fill the circle\n",
    "    radius = 15\n",
    "    \n",
    "    if datatype == 'clusters':\n",
    "        cluster_point = tuple(map(int, [centroid_data[0], centroid_data[1]]))\n",
    "        image = cv2.circle(image, cluster_point, radius, colour, thickness)\n",
    "    \n",
    "    elif datatype == 'noise':     \n",
    "        cluster_point_1 = tuple(map(int, [centroid_data[0]-12, centroid_data[1]-12]))\n",
    "        cluster_point_2 = tuple(map(int, [centroid_data[0]+12, centroid_data[1]+12]))\n",
    "        image = cv2.rectangle(image, cluster_point_1, cluster_point_2, colour, thickness=3)\n",
    "    \n",
    "    elif datatype == 'unassigned':\n",
    "        pass\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_visualization(final_association_dict, list_of_pred_boxes, clusters_on_image, img):\n",
    "\n",
    "    image = cv2.imread(img, cv2.IMREAD_COLOR)\n",
    "    colour_idx = 0\n",
    "    \n",
    "\n",
    "    if final_association_dict['with_cluster']:\n",
    "        for associated_point in final_association_dict['with_cluster']:\n",
    "\n",
    "            # Get box and centroid data\n",
    "            box_data = list_of_pred_boxes[associated_point[1]]\n",
    "\n",
    "            if type(associated_point[0]) == int:\n",
    "                point = associated_point[0]\n",
    "                centroid_data = clusters_on_image['clusters'][point][0]\n",
    "                \n",
    "                # Get Annotations\n",
    "                image = get_drawings(colour_idx, image, box_data, centroid_data, datatype='clusters')\n",
    "                colour_idx += 1\n",
    "            \n",
    "            elif type(associated_point[0]) == list:\n",
    "                for point in associated_point[0]:\n",
    "                    centroid_data = clusters_on_image['clusters'][point][0]\n",
    "                    \n",
    "                    # Get Annotations\n",
    "                    image = get_drawings(colour_idx, image, box_data, centroid_data, datatype='clusters')\n",
    "                colour_idx += 1\n",
    "\n",
    "\n",
    "    if final_association_dict['with_noise']:\n",
    "        for associated_point in final_association_dict['with_noise']:\n",
    "\n",
    "            # Get box and centroid data\n",
    "            box_data = list_of_pred_boxes[associated_point[1]]\n",
    "\n",
    "            if type(associated_point[0]) == int:\n",
    "                point = associated_point[0]\n",
    "                centroid_data = clusters_on_image['noise'][point][0]\n",
    "                \n",
    "                # Get Annotations\n",
    "                image = get_drawings(colour_idx, image, box_data, centroid_data, datatype='noise')\n",
    "                colour_idx += 1\n",
    "            \n",
    "            elif type(associated_point[0]) == list:\n",
    "                for point in associated_point[0]:\n",
    "                    centroid_data = clusters_on_image['noise'][point][0]\n",
    "                    \n",
    "                    # Get Annotations\n",
    "                    image = get_drawings(colour_idx, image, box_data, centroid_data, datatype='noise')\n",
    "                colour_idx += 1\n",
    "\n",
    "\n",
    "    if final_association_dict['unassigned_bbox']:\n",
    "        for non_associated_point in final_association_dict['unassigned_bbox']:\n",
    "            # Get box and centroid data\n",
    "            box_data = list_of_pred_boxes[non_associated_point]\n",
    "            centroid_data = None\n",
    "\n",
    "            # Get Annotations\n",
    "            image = get_drawings(colour_idx, image, box_data, centroid_data, datatype='unassigned')\n",
    "            colour_idx += 1\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inference__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-09-314.png: 416x640 1 car, 75.0ms\n",
      "Speed: 4.0ms preprocess, 75.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      561.83,       521.6,      1225.7,      959.84,     0.95763]], dtype=float32)\n",
      "{'clusters': [[]],\n",
      " 'noise': [[[5.950912952423096, 1.220850944519043, -1.7535979747772217],\n",
      "            [7.412753105163574]],\n",
      "           [[6.699258327484131, 0.916589617729187, -1.7385363578796387],\n",
      "            [8.009551048278809]],\n",
      "           [[8.693258285522461, -0.6627280712127686, -2.348573684692383],\n",
      "            [9.099832534790039]],\n",
      "           [[74.41239929199219, -27.773658752441406, 0.39894238114356995],\n",
      "            [-0.26798269152641296]],\n",
      "           [[79.20195770263672, -11.261544227600098, 2.12113618850708],\n",
      "            [-0.4293433725833893]],\n",
      "           [[98.16716766357422, -11.847762107849121, -1.154921531677246],\n",
      "            [-0.40474042296409607]]]}\n",
      "' association_matrix: None'\n",
      "('noise_association_matrix: [[          0]\\n'\n",
      " ' [          0]\\n'\n",
      " ' [          0]\\n'\n",
      " ' [          0]\\n'\n",
      " ' [          0]\\n'\n",
      " ' [          0]]')\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-09-381.png: 416x640 1 car, 344.0ms\n",
      "Speed: 3.0ms preprocess, 344.0ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      661.21,       495.6,      1252.5,      900.08,     0.96156]], dtype=float32)\n",
      "{'clusters': [[]],\n",
      " 'noise': [[[5.093354225158691, 1.1830286979675293, -2.2629265785217285],\n",
      "            [6.940281391143799]],\n",
      "           [[7.264769077301025, 0.7081881761550903, -1.7079486846923828],\n",
      "            [8.277358055114746]],\n",
      "           [[7.329411029815674, 0.6674264073371887, -1.7291584014892578],\n",
      "            [8.2987060546875]],\n",
      "           [[9.171441078186035, -0.894816517829895, -2.331077814102173],\n",
      "            [9.206526756286621]],\n",
      "           [[19.27054214477539, 13.983783721923828, -0.14120396971702576],\n",
      "            [0.24341514706611633]],\n",
      "           [[70.08948516845703, -38.721500396728516, 0.48748284578323364],\n",
      "            [-0.4454578459262848]],\n",
      "           [[62.663265228271484, -76.5877914428711, -0.0],\n",
      "            [-0.27807825803756714]]]}\n",
      "' association_matrix: None'\n",
      "('noise_association_matrix: [[          0]\\n'\n",
      " ' [          0]\\n'\n",
      " ' [          0]\\n'\n",
      " ' [          0]\\n'\n",
      " ' [          0]\\n'\n",
      " ' [          0]\\n'\n",
      " ' [          0]]')\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-09-481.png: 416x640 1 car, 130.0ms\n",
      "Speed: 4.0ms preprocess, 130.0ms inference, 4.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      811.27,      459.68,      1292.2,      792.34,     0.95464]], dtype=float32)\n",
      "{'clusters': [[[6.8507256507873535, 0.23385874927043915, -2.3311445713043213],\n",
      "               [5.72980260848999, 0.23385874927043915, -2.3334665298461914],\n",
      "               [7.495007038116455]],\n",
      "              [[18.12117576599121, 14.251996040344238, -0.3246823847293854],\n",
      "               [17.49150276184082, 14.251996040344238, 0.07628616690635681],\n",
      "               [0.09964074939489365]]],\n",
      " 'noise': [[[15.22192668914795, 14.509185791015625, -0.8961893320083618],\n",
      "            [-0.18663938343524933]],\n",
      "           [[27.45659065246582, 3.094207525253296, 3.374309778213501],\n",
      "            [0.28833526372909546]],\n",
      "           [[60.33359146118164, -12.427983283996582, -10.454498291015625],\n",
      "            [-0.17080213129520416]],\n",
      "           [[93.52033233642578, -15.113852500915527, 2.54856276512146],\n",
      "            [-0.26481708884239197]]]}\n",
      "' association_matrix: [[          1]\\n [          0]]'\n",
      "('noise_association_matrix: [[          0]\\n'\n",
      " ' [          0]\\n'\n",
      " ' [          0]\\n'\n",
      " ' [          0]]')\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-09-581.png: 416x640 1 car, 109.0ms\n",
      "Speed: 4.0ms preprocess, 109.0ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      872.57,      443.68,      1314.6,      745.37,     0.95264]], dtype=float32)\n",
      "{'clusters': [[[7.339005470275879, -0.19171705842018127, -2.3309154510498047],\n",
      "               [6.587320804595947, -0.19171705842018127, -2.411837100982666],\n",
      "               [8.499577522277832]]],\n",
      " 'noise': [[[17.756484985351562, 14.500992774963379, -0.9391666054725647],\n",
      "            [-0.15974116325378418]]]}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: [[          0]]'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-09-614.png: 416x640 1 car, 100.0ms\n",
      "Speed: 3.0ms preprocess, 100.0ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      900.93,      436.24,      1324.7,      727.88,     0.95564]], dtype=float32)\n",
      "{'clusters': [[[8.13089656829834, -0.1488177329301834, -2.019974946975708],\n",
      "               [7.077520370483398, -0.1488177329301834, -2.4239675998687744],\n",
      "               [8.697218894958496]]],\n",
      " 'noise': [[[16.49056625366211, 14.556048393249512, -0.7151607275009155],\n",
      "            [-0.29447993636131287]]]}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: [[          0]]'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-09-681.png: 416x640 1 car, 75.0ms\n",
      "Speed: 3.0ms preprocess, 75.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      950.97,      420.24,      1339.5,      692.05,     0.95229]], dtype=float32)\n",
      "{'clusters': [[[8.48214340209961, -0.6506607532501221, -2.2715978622436523],\n",
      "               [7.552825927734375, -0.6506607532501221, -2.333630084991455],\n",
      "               [8.836771011352539]]],\n",
      " 'noise': [[[73.26326751708984, -19.05588150024414, -1.3717759847640991],\n",
      "            [0.10994032770395279]],\n",
      "           [[96.08790588378906, 5.941511631011963, 3.1032745838165283],\n",
      "            [-0.10396404564380646]]]}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: [[          0]\\n [          0]]'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-09-714.png: 416x640 1 car, 77.0ms\n",
      "Speed: 3.0ms preprocess, 77.0ms inference, 4.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      974.04,      414.19,      1347.9,       675.1,      0.9564]], dtype=float32)\n",
      "{'clusters': [[[9.064905166625977, -0.5504707098007202, -2.0541274547576904],\n",
      "               [7.98128080368042, -0.5504707098007202, -2.4291388988494873],\n",
      "               [8.921479225158691]]],\n",
      " 'noise': [[[16.680564880371094, 14.537944793701172, -0.5486180782318115],\n",
      "            [-0.24333305656909943]]]}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: [[          0]]'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-09-781.png: 416x640 1 car, 142.0ms\n",
      "Speed: 4.0ms preprocess, 142.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1014.2,      403.77,      1361.4,      649.67,     0.95358]], dtype=float32)\n",
      "{'clusters': [[[9.688504219055176, -1.177303433418274, -2.002965211868286],\n",
      "               [8.52904224395752, -1.177303433418274, -2.4082183837890625],\n",
      "               [9.115325927734375]]],\n",
      " 'noise': [[[18.304784774780273, 13.815544128417969, -0.3994672894477844],\n",
      "            [-0.12717212736606598]]]}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: [[          0]]'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-09-814.png: 416x640 1 car, 90.0ms\n",
      "Speed: 3.0ms preprocess, 90.0ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1033.6,      400.33,      1367.9,      638.51,     0.95641]], dtype=float32)\n",
      "{'clusters': [[[10.046957015991211, -1.306298017501831, -1.9556008577346802],\n",
      "               [8.817811012268066, -1.306298017501831, -2.377025842666626],\n",
      "               [9.149776458740234]]],\n",
      " 'noise': [[[8.080953598022461, -5.066962242126465, -2.4828507900238037],\n",
      "            [-0.11448517441749573]],\n",
      "           [[32.001426696777344, 0.269467830657959, 2.1498312950134277],\n",
      "            [-0.1100400909781456]],\n",
      "           [[96.09803771972656, 5.7525200843811035, 3.0052106380462646],\n",
      "            [0.10215281695127487]]]}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: [[          0]\\n [          0]\\n [          0]]'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-09-881.png: 416x640 1 car, 81.0ms\n",
      "Speed: 3.0ms preprocess, 81.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1068.3,      389.59,      1380.1,       616.8,     0.96117]], dtype=float32)\n",
      "{'clusters': [[[11.31710147857666, -1.896863579750061, -1.9038814306259155],\n",
      "               [10.023432731628418, -1.896863579750061, -1.698752999305725],\n",
      "               [9.261811256408691]]],\n",
      " 'noise': []}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: None'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-09-914.png: 416x640 1 car, 119.0ms\n",
      "Speed: 2.0ms preprocess, 119.0ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1083.2,      384.62,      1385.9,      604.95,     0.95815]], dtype=float32)\n",
      "{'clusters': [[[11.106590270996094, -2.352241039276123, -1.8097164630889893],\n",
      "               [10.069042205810547, -2.352241039276123, -1.8529521226882935],\n",
      "               [9.390746116638184]]],\n",
      " 'noise': [[[18.659055709838867, 8.313188552856445, 1.6519129276275635],\n",
      "            [0.15453459322452545]],\n",
      "           [[18.234376907348633, 13.747331619262695, -0.21227024495601654],\n",
      "            [-0.2533758580684662]]]}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: [[          0]\\n [          0]]'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-09-981.png: 416x640 1 car, 105.0ms\n",
      "Speed: 4.0ms preprocess, 105.0ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1113.7,      376.91,      1398.7,      585.24,     0.95644]], dtype=float32)\n",
      "{'clusters': [[[11.76921558380127, -2.041935443878174, -1.6340103149414062],\n",
      "               [11.2212553024292, -2.041935443878174, -1.6710624694824219],\n",
      "               [9.292275428771973]]],\n",
      " 'noise': [[[18.015432357788086, 14.326065063476562, -0.1632491797208786],\n",
      "            [-0.11643427610397339]],\n",
      "           [[61.88020706176758, 50.53420639038086, 2.3313050270080566],\n",
      "            [-0.14413484930992126]]]}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: [[          0]\\n [          0]]'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-10-081.png: 416x640 1 car, 78.0ms\n",
      "Speed: 4.0ms preprocess, 78.0ms inference, 4.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1150.4,      365.72,      1413.5,      558.78,     0.94825]], dtype=float32)\n",
      "{'clusters': [[[12.328808784484863, -2.0602355003356934, -1.7992115020751953],\n",
      "               [11.561483383178711, -2.0602355003356934, -2.322923183441162],\n",
      "               [9.16873836517334]]],\n",
      " 'noise': [[[14.619645118713379, -4.735604286193848, -2.823127031326294],\n",
      "            [9.46642017364502]],\n",
      "           [[17.10346794128418, 14.976815223693848, -0.9131223559379578],\n",
      "            [-0.19424358010292053]]]}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: [[          0]\\n [          0]]'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-10-114.png: 416x640 1 car, 73.0ms\n",
      "Speed: 3.0ms preprocess, 73.0ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1163.5,      363.03,      1418.7,      550.87,     0.94738]], dtype=float32)\n",
      "{'clusters': [[[12.641554832458496, -2.9912045001983643, -1.8946481943130493],\n",
      "               [11.539402961730957, -2.9912045001983643, -2.2817862033843994],\n",
      "               [9.343708992004395]]],\n",
      " 'noise': [[[10.18274974822998, -5.954841136932373, -2.298734664916992],\n",
      "            [-0.1061224639415741]]]}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: [[          0]]'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-10-181.png: 416x640 1 car, 69.0ms\n",
      "Speed: 3.0ms preprocess, 69.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1183.9,      356.83,      1427.5,      535.25,     0.95108]], dtype=float32)\n",
      "{'clusters': [[[13.122644424438477, -3.1117217540740967, -1.8936712741851807],\n",
      "               [11.955273628234863, -3.1117217540740967, -2.3185863494873047],\n",
      "               [9.311132431030273]]],\n",
      " 'noise': [[[26.437164306640625, 13.276387214660645, -1.4683632850646973],\n",
      "            [-0.14805403351783752]]]}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: [[          0]]'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-10-281.png: 416x640 1 car, 79.1ms\n",
      "Speed: 4.0ms preprocess, 79.1ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1214.5,      347.83,      1438.2,      516.04,     0.94919]], dtype=float32)\n",
      "{'clusters': [[[14.401947975158691, -3.82116961479187, -2.1040468215942383],\n",
      "               [12.783822059631348, -3.82116961479187, -2.335627794265747],\n",
      "               [9.298259735107422]]],\n",
      " 'noise': [[[15.12397289276123, 13.798396110534668, 0.13603371381759644],\n",
      "            [-0.2079811990261078]],\n",
      "           [[28.03415298461914, 5.432541370391846, 1.1971932649612427],\n",
      "            [-0.13946063816547394]]]}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: [[          0]\\n [          0]]'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-10-381.png: 416x640 1 car, 102.0ms\n",
      "Speed: 3.0ms preprocess, 102.0ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1239.7,      340.72,      1449.9,      498.84,     0.95649]], dtype=float32)\n",
      "{'clusters': [[[14.916833877563477, -3.925894021987915, -1.781277060508728],\n",
      "               [14.01702880859375, -3.925894021987915, 0.28798502683639526],\n",
      "               [9.225067138671875]]],\n",
      " 'noise': []}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: None'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-10-414.png: 416x640 1 car, 91.0ms\n",
      "Speed: 3.0ms preprocess, 91.0ms inference, 5.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1247.1,      339.23,      1453.2,      493.54,     0.94968]], dtype=float32)\n",
      "{'clusters': [[[15.942134857177734, -4.783339023590088, -2.227024793624878],\n",
      "               [14.433452606201172, -4.783339023590088, -1.7012851238250732],\n",
      "               [9.278621673583984]]],\n",
      " 'noise': []}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: None'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-10-481.png: 416x640 1 car, 90.0ms\n",
      "Speed: 3.0ms preprocess, 90.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1263.1,      333.67,      1459.2,      483.83,     0.95628]], dtype=float32)\n",
      "{'clusters': [[[15.525748252868652, -4.258547306060791, -1.4099702835083008],\n",
      "               [14.807394027709961, -4.258547306060791, 0.543803870677948],\n",
      "               [9.2024564743042]]],\n",
      " 'noise': []}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: None'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-10-581.png: 416x640 1 car, 83.0ms\n",
      "Speed: 4.0ms preprocess, 83.0ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1282.5,       328.3,        1468,      469.92,     0.94665]], dtype=float32)\n",
      "{'clusters': [[[17.016429901123047, -5.234700679779053, -2.1307339668273926],\n",
      "               [15.94360065460205, -5.234700679779053, -2.711240291595459],\n",
      "               [9.113197326660156]]],\n",
      " 'noise': []}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: None'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-10-681.png: 416x640 1 car, 74.0ms\n",
      "Speed: 2.0ms preprocess, 74.0ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1299.4,      323.26,      1477.2,      457.04,     0.94673]], dtype=float32)\n",
      "{'clusters': [[]],\n",
      " 'noise': [[[16.973342895507812, -4.304269313812256, -2.500875473022461],\n",
      "            [9.0267915725708]],\n",
      "           [[17.655330657958984, -4.873157978057861, -1.5918105840682983],\n",
      "            [9.112473487854004]],\n",
      "           [[19.0661678314209, 7.051680564880371, 1.8905190229415894],\n",
      "            [-0.29131025075912476]],\n",
      "           [[17.36618995666504, 14.138673782348633, 0.10218015313148499],\n",
      "            [0.10591727495193481]],\n",
      "           [[17.269283294677734, 15.003769874572754, -1.3137115240097046],\n",
      "            [-0.11464259028434753]]]}\n",
      "' association_matrix: None'\n",
      "('noise_association_matrix: [[          0]\\n'\n",
      " ' [          0]\\n'\n",
      " ' [          0]\\n'\n",
      " ' [          0]\\n'\n",
      " ' [          0]]')\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-10-714.png: 416x640 1 car, 75.0ms\n",
      "Speed: 3.0ms preprocess, 75.0ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1305.7,      321.94,      1479.1,      453.74,       0.951]], dtype=float32)\n",
      "{'clusters': [[[18.79165267944336, -5.97273588180542, -2.2859082221984863],\n",
      "               [18.058935165405273, -5.97273588180542, -1.3993722200393677],\n",
      "               [9.149032592773438]]],\n",
      " 'noise': [[[16.7947998046875, 14.909360885620117, -1.366438388824463],\n",
      "            [-0.1328916698694229]]]}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: [[          0]]'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-10-781.png: 416x640 1 car, 72.0ms\n",
      "Speed: 3.0ms preprocess, 72.0ms inference, 4.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1316.1,      318.79,      1484.1,      446.78,     0.93693]], dtype=float32)\n",
      "{'clusters': [[[18.67035675048828, -5.759471893310547, -2.304765224456787],\n",
      "               [17.7215576171875, -5.759471893310547, -2.498699426651001],\n",
      "               [9.018638610839844]]],\n",
      " 'noise': [[[17.254894256591797, 14.404515266418457, 0.18934553861618042],\n",
      "            [0.1383175253868103]]]}\n",
      "' association_matrix: [[          1]]'\n",
      "'noise_association_matrix: [[          0]]'\n",
      "\n",
      "image 1/1 C:\\Dk\\Projects\\Team Project\\Dataset\\INFRA-3DRC-Dataset\\INFRA-3DRC_scene-15\\camera_01\\camera_01__data\\camera_01__2023-06-02-21-28-10-881.png: 416x640 1 car, 95.0ms\n",
      "Speed: 4.0ms preprocess, 95.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "array([[          3,      1331.4,      315.82,      1489.3,      436.52,     0.93186]], dtype=float32)\n",
      "{'clusters': [[[18.95256805419922, -6.110057830810547, -0.9441754817962646],\n",
      "               [18.39153289794922, -6.110057830810547, -2.716825246810913],\n",
      "               [8.97148609161377]]],\n",
      " 'noise': [[[18.826021194458008, 13.584750175476074, 4.630882263183594],\n",
      "            [-0.1451193392276764]]]}\n",
      "' association_matrix: [[          0]]'\n",
      "'noise_association_matrix: [[          0]]'\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3202], line 19\u001b[0m\n\u001b[0;32m     14\u001b[0m sensor_calibration_dict \u001b[38;5;241m=\u001b[39m get_sensor_calibration_dict(calibration_file)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, pcd \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(scene_image, scene_pcd):\n\u001b[0;32m     17\u001b[0m     \n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# YOLO prediction\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43myolo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     list_of_pred_boxes \u001b[38;5;241m=\u001b[39m class_box_generator_for_pred(results)\n\u001b[0;32m     21\u001b[0m     pprint(list_of_pred_boxes)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ultralytics\\engine\\model.py:452\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:248\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 248\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    141\u001b[0m )\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ultralytics\\nn\\autobackend.py:447\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 447\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:89\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:107\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:128\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 128\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    129\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ultralytics\\nn\\modules\\head.py:74\u001b[0m, in \u001b[0;36mDetect.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     dbox \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_bboxes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfl(box), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchors\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides\n\u001b[1;32m---> 74\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((dbox, \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport \u001b[38;5;28;01melse\u001b[39;00m (y, x)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1])\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "\n",
    "my_image = fig.add_subplot(gs[0])\n",
    "my_plot = fig.add_subplot(gs[1])\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "# total_box = []\n",
    "# unassigned_case = []\n",
    "\n",
    "# for scene_image, scene_pcd, calibration_file in zip(image_list, pcd_list, calibration_list):\n",
    "sensor_calibration_dict = get_sensor_calibration_dict(calibration_file)\n",
    "\n",
    "for img, pcd in zip(scene_image, scene_pcd):\n",
    "    \n",
    "    # YOLO prediction\n",
    "    results = yolo_model.predict(img)\n",
    "    list_of_pred_boxes = class_box_generator_for_pred(results)\n",
    "    pprint(list_of_pred_boxes)\n",
    "    \n",
    "    # Calculate total no of boxes in all the images\n",
    "    # total_box.append(len(list_of_pred_boxes))\n",
    "    # print(total_box)\n",
    "\n",
    "    # Cluster Formation\n",
    "    db_scan = my_custom_dbscan(eps1=0.1, eps2=0.250, min_samples=2)\n",
    "    clusters_on_radar = db_scan.process_pcd_files(pcd)\n",
    "    pprint(clusters_on_radar)\n",
    "\n",
    "    # Bbox point on the Ground Plane\n",
    "    image_on_ground = []\n",
    "    for result in list_of_pred_boxes:\n",
    "        cls = result[0]\n",
    "        bbox = list(result[1:5])\n",
    "        bottom_center_point = list(((bbox[2] + bbox[0]) / 2, bbox[3]))\n",
    "        image_point_on_ground = homography(bottom_center_point, sensor_calibration_dict)\n",
    "        image_on_ground.append([[cls], list(image_point_on_ground)])\n",
    "    # pprint(image_on_ground)\n",
    "\n",
    "    # Radar Dictionaries on different Planes\n",
    "    clusters_on_ground = radar_to_ground(clusters_on_radar, sensor_calibration_dict)\n",
    "    # pprint(f\"cluster on ground: {clusters_on_ground} \")\n",
    "\n",
    "    clusters_on_image = radar_to_camera(clusters_on_radar, sensor_calibration_dict)\n",
    "    # pprint(f\"cluster on image: {clusters_on_image} \")\n",
    "\n",
    "    \n",
    "    # Fiter Cases with Association Matrix\n",
    "    association_matrix = get_association_matrix(list_of_pred_boxes, clusters_on_image, datatype= 'clusters') \n",
    "    pprint(f\" association_matrix: {association_matrix}\")\n",
    "\n",
    "    association_list = {\"associated\": [], \"non_associated_bbox\": []}\n",
    "\n",
    "    if association_matrix is not None:\n",
    "        filtered_cases = get_filtered_cases(association_matrix,  datatype='clusters')\n",
    "        # pprint(f\" filtered_cases: {filtered_cases}\")\n",
    "\n",
    "        # Association: One to One\n",
    "        association_list = get_one_to_one_association(filtered_cases, association_list)\n",
    "        # pprint(f\" after one to one: {association_list}\")\n",
    "\n",
    "        # Association: One to Many\n",
    "        association_list = get_one_to_many_association(filtered_cases, clusters_on_ground, image_on_ground, association_list=association_list, datatype= 'clusters')\n",
    "        # pprint(f\" after one to many: {association_list}\")\n",
    "\n",
    "        # Association: Many to One\n",
    "        association_list = get_many_to_one_association(filtered_cases, association_list, clusters_on_ground, image_on_ground, datatype='clusters')\n",
    "        # pprint(f\" after many to one: {association_list}\")\n",
    "\n",
    "        # Association: Many to Many\n",
    "        # association_list = get_many_to_many_association(filtered_cases, association_list, clusters_on_ground, image_on_ground, datatype='clusters')\n",
    "        # pprint(f\" after many to many: {association_list}\")\n",
    "\n",
    "\n",
    "        # Calculate Unassigned bbox without any clusters inside\n",
    "        # unassigned_case.append(len(association_list[\"non_associated_bbox\"][0]))\n",
    "        # print(unassigned_case)\n",
    "\n",
    "    noise_association_matrix = get_association_matrix(list_of_pred_boxes, clusters_on_image, association_list=association_list, datatype='noise')\n",
    "    pprint(f\"noise_association_matrix: {noise_association_matrix}\")\n",
    "\n",
    "    noise_association_list = {\"associated\": [], \"non_associated_bbox\": []}\n",
    "\n",
    "    if noise_association_matrix is not None:\n",
    "        noise_filtered_cases = get_filtered_cases(noise_association_matrix, datatype='noise', association_list=association_list)\n",
    "        # pprint(f\" noise filtered cases: {noise_filtered_cases}\")\n",
    "\n",
    "        # Association: One to One\n",
    "        noise_association_list = get_one_to_one_association(noise_filtered_cases, noise_association_list)\n",
    "        # pprint(f\" after one to one: {noise_association_list}\")\n",
    "\n",
    "        # Association: One to Many\n",
    "        noise_association_list = get_one_to_many_association(noise_filtered_cases, clusters_on_ground, image_on_ground, association_list=noise_association_list, datatype= 'noise')\n",
    "        # pprint(f\" after one to many: {noise_association_list}\")\n",
    "\n",
    "        # Association: Many to One    \n",
    "        noise_association_list = get_many_to_one_association(noise_filtered_cases, noise_association_list, clusters_on_ground, image_on_ground, datatype='noise')\n",
    "        # pprint(f\" after Many to One: {noise_association_list}\")\n",
    "\n",
    "\n",
    "    # Final Association List\n",
    "    final_association_dict = {'with_cluster': [], 'with_noise': [], 'unassigned_bbox': []}\n",
    "    final_association_dict['with_cluster'].extend(association_list['associated'])\n",
    "    final_association_dict['with_noise'].extend(noise_association_list['associated'])\n",
    "    final_association_dict['unassigned_bbox'].extend(noise_association_list['non_associated_bbox'])\n",
    "    \n",
    "    # pprint(final_association_dict)\n",
    "\n",
    "    \n",
    "    # Visualization:\n",
    "    my_image.clear()\n",
    "    my_plot.clear()\n",
    "\n",
    "    # Points on Ground Plane\n",
    "    plot = camera_plotting(image_on_ground, my_plot)\n",
    "    plot2 = radar_plotting(clusters_on_ground, plot)\n",
    "\n",
    "    # Association Visualization on Image Plane\n",
    "    image_visualize = get_image_visualization(final_association_dict, list_of_pred_boxes, clusters_on_image, img)\n",
    "    # cv2.imshow('on image plane', image_visualize)\n",
    "    image_visualize = cv2.cvtColor(image_visualize, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    my_image.imshow(image_visualize)\n",
    "    my_image.set_title('Image')\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    # time.sleep(100)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "\n",
    "# print(np.sum(total_box))\n",
    "# print(np.sum(unassigned_case))\n",
    "\n",
    "\n",
    "plt.ioff()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
